
import json
import os
import numpy as np
from collections import Counter

def analyze_dataset():
   
    
    print("=" * 60)
    print("ðŸ” ANÃLISIS DEL DATASET")
    print("=" * 60)
    
    data_path = os.path.join(os.path.dirname(__file__), 'data', 'combined_training_data.json')
    
    if not os.path.exists(data_path):
        print(f"âŒ No se encontrÃ³: {data_path}")
        return
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Manejar ambos formatos
    if isinstance(data, list):
        all_data = data
        print("ðŸ“„ Formato: Lista directa")
    else:
        training = data.get('training_data', [])
        feedback = data.get('feedback_data', [])
        pending = data.get('pending_feedback', [])
        all_data = training + feedback + pending
        print(f"ðŸ“„ Formato: Objeto estructurado")
        print(f"   - Training data: {len(training)}")
        print(f"   - Feedback data: {len(feedback)}")
        print(f"   - Pending: {len(pending)}")
    
    print(f"\nðŸ“Š Total de ejemplos: {len(all_data)}")
    
    if len(all_data) == 0:
        print("âŒ Dataset vacÃ­o!")
        return
    
    # Analizar ratings
    print("\n" + "=" * 60)
    print("ðŸ“ˆ DISTRIBUCIÃ“N DE RATINGS")
    print("=" * 60)
    
    ratings = []
    for item in all_data:
        if isinstance(item, dict) and 'rating' in item:
            ratings.append(item['rating'])
    
    if not ratings:
        print("âš ï¸ No se encontraron ratings en los datos")
        print("\nâ„¹ï¸ Estructura de ejemplo encontrada:")
        if all_data:
            print(json.dumps(all_data[0], indent=2)[:300])
        return
    
    ratings = np.array(ratings)
    
    print(f"\nðŸ“Š EstadÃ­sticas de ratings:")
    print(f"   - MÃ­nimo: {ratings.min():.3f}")
    print(f"   - MÃ¡ximo: {ratings.max():.3f}")
    print(f"   - Promedio: {ratings.mean():.3f}")
    print(f"   - Mediana: {np.median(ratings):.3f}")
    print(f"   - Desv. EstÃ¡ndar: {ratings.std():.3f}")
    
    # AnÃ¡lisis de distribuciÃ³n
    print(f"\nðŸ“Š DistribuciÃ³n por rangos:")
    ranges = [
        (0.0, 0.3, "Malo (0.0-0.3)"),
        (0.3, 0.5, "Regular (0.3-0.5)"),
        (0.5, 0.7, "Bueno (0.5-0.7)"),
        (0.7, 0.9, "Muy bueno (0.7-0.9)"),
        (0.9, 1.1, "Excelente (0.9-1.0)")
    ]
    
    for min_r, max_r, label in ranges:
        count = np.sum((ratings >= min_r) & (ratings < max_r))
        percentage = (count / len(ratings)) * 100
        bar = "â–ˆ" * int(percentage / 5)
        print(f"   {label:25s} {bar:20s} {count:3d} ({percentage:5.1f}%)")
    
    # AnÃ¡lisis de clases (bueno/malo)
    print(f"\nðŸ“Š ClasificaciÃ³n binaria (threshold=0.7):")
    buenos = np.sum(ratings >= 0.7)
    malos = len(ratings) - buenos
    print(f"   - Buenos (â‰¥0.7): {buenos} ({buenos/len(ratings)*100:.1f}%)")
    print(f"   - Malos (<0.7): {malos} ({malos/len(ratings)*100:.1f}%)")
    
    # AnÃ¡lisis de variabilidad
    print(f"\nðŸŽ¯ AnÃ¡lisis de variabilidad:")
    unique_ratings = len(np.unique(ratings))
    print(f"   - Valores Ãºnicos de rating: {unique_ratings}")
    
    if ratings.std() < 0.1:
        print("   âš ï¸ PROBLEMA: Muy poca variabilidad en los datos")
        print("   ðŸ’¡ SoluciÃ³n: Agrega ejemplos con ratings mÃ¡s diversos")
    elif buenos < 5 or malos < 5:
        print("   âš ï¸ PROBLEMA: Dataset desbalanceado")
        print(f"   ðŸ’¡ SoluciÃ³n: Necesitas al menos 5 ejemplos de cada clase")
    else:
        print("   âœ… Variabilidad adecuada")
    
    # Analizar features
    print("\n" + "=" * 60)
    print("ðŸŽ¨ ANÃLISIS DE CARACTERÃSTICAS")
    print("=" * 60)
    
    sample_input = None
    for item in all_data:
        if isinstance(item, dict) and 'input' in item:
            sample_input = item['input']
            break
    
    if sample_input:
        print("\nðŸ“ Estructura de input encontrada:")
        print(json.dumps(sample_input, indent=2, ensure_ascii=False))
        
        # Analizar quÃ© features se estÃ¡n usando
        print(f"\nðŸ”‘ Features detectados:")
        for key, value in sample_input.items():
            value_type = type(value).__name__
            value_preview = str(value)[:50]
            print(f"   - {key}: {value_type} = {value_preview}")
    
    # Recomendaciones
    print("\n" + "=" * 60)
    print("ðŸ’¡ RECOMENDACIONES")
    print("=" * 60)
    
    issues = []
    
    if len(all_data) < 50:
        issues.append("Dataset pequeÃ±o: Genera mÃ¡s datos sintÃ©ticos (recomendado: 100+)")
    
    if ratings.std() < 0.15:
        issues.append("Poca variabilidad: Agrega ejemplos con ratings mÃ¡s extremos (0.0-0.3 y 0.8-1.0)")
    
    if buenos < 10 or malos < 10:
        issues.append("Clases desbalanceadas: Balancea los ejemplos buenos y malos")
    
    if unique_ratings < 10:
        issues.append("Pocos valores Ãºnicos: Usa ratings mÃ¡s variados (no solo 0.5, 0.7, etc.)")
    
    if issues:
        for i, issue in enumerate(issues, 1):
            print(f"\n{i}. âš ï¸ {issue}")
    else:
        print("\nâœ… Tu dataset parece estar bien estructurado")
        print("   El problema puede estar en el DataProcessor o en la arquitectura del modelo")
    
    print("\n" + "=" * 60)

if __name__ == "__main__":
    analyze_dataset()